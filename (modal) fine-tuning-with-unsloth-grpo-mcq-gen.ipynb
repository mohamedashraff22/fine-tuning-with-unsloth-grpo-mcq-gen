{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCQ Generation Fine-tuning with Unsloth GRPO\n",
        "**Fine-tune Qwen2.5-3B-Instruct to generate Multiple Choice Questions from text context**\n",
        "\n",
        "This notebook fine-tunes a model to:\n",
        "- Take text passages as input\n",
        "- Generate MCQ questions with 4 options and correct answer\n",
        "- Output structured JSON format\n",
        "\n",
        "Based on EleutherAI/RACE dataset format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtransformers==4.56.2                                                          \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mfilelock==3.20.0                                                              \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhuggingface-hub==0.34.4                                                       \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mnumpy==2.1.2                                                                  \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpackaging==25.0                                                               \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpyyaml==6.0.2                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mregex==2025.9.1                                                               \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mrequests==2.32.5                                                              \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtokenizers==0.22.0                                                            \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2msafetensors==0.6.2                                                            \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtqdm==4.67.1                                                                  \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mfsspec==2024.6.1                                                              \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhf-xet==1.1.9                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhf-xet==1.1.9                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mcharset-normalizer==3.4.3                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2midna==3.10                                                                    \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2murllib3==2.5.0                                                                \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mcertifi==2024.8.30                                                            \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 57ms\u001b[0m\u001b[0m\r\n",
            "\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.07 MiB           \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 16.00 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 32.00 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 48.00 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 62.05 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 78.05 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 94.05 KiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 110.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 126.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 142.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 158.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 174.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 190.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 206.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 222.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 238.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m 254.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 590.05 KiB/11.07 MiB        \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 1.37 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m-----\u001b[30m\u001b[2m-------------------------\u001b[0m\u001b[0m 2.20 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m--------\u001b[30m\u001b[2m----------------------\u001b[0m\u001b[0m 3.00 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m----------\u001b[30m\u001b[2m--------------------\u001b[0m\u001b[0m 3.73 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m------------\u001b[30m\u001b[2m------------------\u001b[0m\u001b[0m 4.59 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m--------------\u001b[30m\u001b[2m----------------\u001b[0m\u001b[0m 5.38 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m----------------\u001b[30m\u001b[2m--------------\u001b[0m\u001b[0m 6.17 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m------------------\u001b[30m\u001b[2m------------\u001b[0m\u001b[0m 6.95 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m--------------------\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 7.68 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m-----------------------\u001b[30m\u001b[2m-------\u001b[0m\u001b[0m 8.54 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m-------------------------\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 9.31 MiB/11.07 MiB          \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\r\n",
            "\u001b[2mtransformers        \u001b[0m \u001b[32m---------------------------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 10.15 MiB/11.07 MiB         \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 694ms\u001b[0m\u001b[0m\r\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 679ms\u001b[0m\u001b[0m\r\n",
            "â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/1] \u001b[2mtransformers==4.56.2                                 \u001b[0m\r\u001b[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [1/1] \u001b[2mtransformers==4.56.2                                 \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 256ms\u001b[0m\u001b[0m\r\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.0\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\r\n",
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\r\n",
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\r\n",
            "âœ“ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
        "\n",
        "# Install dependencies for Modal\n",
        "!pip install --upgrade -q uv\n",
        "\n",
        "try:\n",
        "    import numpy, PIL\n",
        "    get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    get_pil = f\"pillow=={PIL.__version__}\"\n",
        "except:\n",
        "    get_numpy = \"numpy\"\n",
        "    get_pil = \"pillow\"\n",
        "\n",
        "import subprocess\n",
        "try:\n",
        "    is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "except:\n",
        "    is_t4 = False\n",
        "\n",
        "get_vllm = \"vllm==0.9.2\" if is_t4 else \"vllm==0.10.2\"\n",
        "get_triton = \"triton==3.2.0\" if is_t4 else \"triton\"\n",
        "\n",
        "!uv pip install -q --system unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "!uv pip install -q --system {get_triton}\n",
        "!uv pip install --system transformers==4.56.2\n",
        "!uv pip install --system --no-deps trl==0.22.2\n",
        "!uv pip install --system pydantic datasets\n",
        "\n",
        "print(\"âœ“ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 11-16 19:40:14 [__init__.py:216] Automatically detected platform cuda.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 11-16 19:40:23 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 11-16 19:40:23 [vllm_utils.py:730] Unsloth: Patching vLLM v0 graph capture\n",
            "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.251 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.0. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 64.63%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.25 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1536. Num Sequences = 368.\n",
            "Unsloth: vLLM's KV Cache can use up to 48.8 GB. Also swap space = 6 GB.\n",
            "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
            "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
            "INFO 11-16 19:40:26 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 1536, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6462930630816577, 'max_num_batched_tokens': 2048, 'max_num_seqs': 368, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit'}\n",
            "INFO 11-16 19:40:41 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-16 19:40:41 [__init__.py:1815] Using max model len 1536\n",
            "WARNING 11-16 19:40:41 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
            "INFO 11-16 19:40:42 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "WARNING 11-16 19:40:42 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 11-16 19:40:44 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "INFO 11-16 19:40:44 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 11-16 19:40:44 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 11-16 19:40:44 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W1116 19:40:44.414189312 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-16 19:40:44 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "INFO 11-16 19:40:45 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "INFO 11-16 19:40:45 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 11-16 19:40:45 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
            "INFO 11-16 19:40:46 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d8b6208d1b4cf99e1e8a00b4452444",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ec107248266453db090d4a7953c018e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-16 19:40:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
            "INFO 11-16 19:40:47 [gpu_model_runner.py:2392] Model loading took 2.4392 GiB and 2.177015 seconds\n",
            "INFO 11-16 19:41:04 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ed5498e051/rank_0_0/backbone for vLLM's torch.compile\n",
            "INFO 11-16 19:41:04 [backends.py:550] Dynamo bytecode transform time: 15.79 s\n",
            "INFO 11-16 19:41:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.596 s\n",
            "INFO 11-16 19:41:18 [monitor.py:34] torch.compile takes 15.79 s in total\n",
            "INFO 11-16 19:41:19 [gpu_worker.py:298] Available KV cache memory: 46.75 GiB\n",
            "INFO 11-16 19:41:20 [kv_cache_utils.py:864] GPU KV cache size: 1,361,744 tokens\n",
            "INFO 11-16 19:41:20 [kv_cache_utils.py:868] Maximum concurrency for 1,536 tokens per request: 886.55x\n",
            "INFO 11-16 19:41:20 [vllm_utils.py:705] Unsloth: Running patched vLLM v1 `capture_model`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|                          | 0/67 [00:00<?, ?it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|â–Ž                 | 1/67 [00:00<00:30,  2.18it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Œ                 | 2/67 [00:00<00:24,  2.63it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–Š                 | 3/67 [00:01<00:22,  2.89it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–ˆ                 | 4/67 [00:01<00:20,  3.02it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–ˆâ–Ž                | 5/67 [00:01<00:19,  3.11it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–ˆâ–Œ                | 6/67 [00:02<00:19,  3.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–ˆâ–‰                | 7/67 [00:02<00:18,  3.20it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–ˆâ–               | 8/67 [00:02<00:18,  3.22it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–ˆâ–               | 9/67 [00:02<00:17,  3.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–ˆâ–Œ              | 10/67 [00:03<00:17,  3.22it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–ˆâ–Š              | 11/67 [00:03<00:17,  3.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–ˆâ–ˆ              | 12/67 [00:03<00:16,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–ˆâ–ˆâ–Ž             | 13/67 [00:04<00:16,  3.25it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆâ–ˆâ–Œ             | 14/67 [00:04<00:16,  3.21it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–ˆâ–Š             | 15/67 [00:04<00:16,  3.20it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–ˆâ–ˆ             | 16/67 [00:05<00:16,  3.15it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 17/67 [00:05<00:16,  3.07it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 18/67 [00:05<00:15,  3.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 19/67 [00:06<00:15,  3.11it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 20/67 [00:06<00:15,  3.12it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 21/67 [00:06<00:14,  3.14it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 22/67 [00:07<00:14,  3.15it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 23/67 [00:07<00:14,  3.12it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 24/67 [00:07<00:13,  3.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 25/67 [00:08<00:13,  3.10it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 26/67 [00:08<00:13,  3.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 27/67 [00:08<00:12,  3.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 28/67 [00:08<00:12,  3.10it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 29/67 [00:09<00:12,  3.12it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 30/67 [00:09<00:11,  3.11it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 31/67 [00:09<00:11,  3.11it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 32/67 [00:10<00:11,  3.15it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 33/67 [00:10<00:10,  3.14it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 34/67 [00:10<00:10,  3.13it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 35/67 [00:11<00:10,  3.14it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 36/67 [00:11<00:09,  3.13it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 37/67 [00:11<00:09,  3.16it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 38/67 [00:12<00:09,  2.98it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 39/67 [00:12<00:09,  3.02it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 40/67 [00:12<00:08,  3.04it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 41/67 [00:13<00:08,  3.10it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 42/67 [00:13<00:07,  3.13it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 43/67 [00:13<00:07,  3.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/67 [00:14<00:07,  3.21it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/67 [00:14<00:06,  3.22it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 46/67 [00:14<00:06,  3.25it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/67 [00:15<00:06,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 48/67 [00:15<00:05,  3.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/67 [00:15<00:05,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 50/67 [00:15<00:05,  3.21it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 51/67 [00:16<00:05,  3.19it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 52/67 [00:16<00:04,  3.21it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 53/67 [00:16<00:04,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 54/67 [00:17<00:04,  3.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 55/67 [00:17<00:03,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 56/67 [00:17<00:03,  3.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57/67 [00:18<00:03,  3.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 58/67 [00:18<00:02,  3.25it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 59/67 [00:18<00:02,  3.25it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 60/67 [00:19<00:02,  3.20it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 61/67 [00:19<00:01,  3.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 62/67 [00:19<00:01,  3.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 63/67 [00:20<00:01,  3.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 64/67 [00:20<00:00,  3.07it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 65/67 [00:20<00:00,  3.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 66/67 [00:21<00:00,  3.10it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.07it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.14it/s]\n",
            "\rCapturing CUDA graphs (decode, FULL):   0%|                                             | 0/49 [00:00<?, ?it/s]\rCapturing CUDA graphs (decode, FULL):   2%|â–Š                                    | 1/49 [00:00<00:19,  2.42it/s]\rCapturing CUDA graphs (decode, FULL):   4%|â–ˆâ–Œ                                   | 2/49 [00:00<00:16,  2.86it/s]\rCapturing CUDA graphs (decode, FULL):   6%|â–ˆâ–ˆâ–Ž                                  | 3/49 [00:01<00:15,  3.06it/s]\rCapturing CUDA graphs (decode, FULL):   8%|â–ˆâ–ˆâ–ˆ                                  | 4/49 [00:01<00:14,  3.16it/s]\rCapturing CUDA graphs (decode, FULL):  10%|â–ˆâ–ˆâ–ˆâ–Š                                 | 5/49 [00:01<00:13,  3.21it/s]\rCapturing CUDA graphs (decode, FULL):  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 6/49 [00:01<00:13,  3.21it/s]\rCapturing CUDA graphs (decode, FULL):  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                               | 7/49 [00:02<00:13,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 8/49 [00:02<00:12,  3.22it/s]\rCapturing CUDA graphs (decode, FULL):  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 9/49 [00:02<00:12,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 10/49 [00:03<00:12,  3.22it/s]\rCapturing CUDA graphs (decode, FULL):  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 11/49 [00:03<00:11,  3.22it/s]\rCapturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 12/49 [00:03<00:11,  3.18it/s]\rCapturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 13/49 [00:04<00:11,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 14/49 [00:04<00:10,  3.18it/s]\rCapturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 15/49 [00:04<00:10,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 16/49 [00:05<00:10,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 17/49 [00:05<00:10,  3.15it/s]\rCapturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 18/49 [00:05<00:09,  3.11it/s]\rCapturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 19/49 [00:06<00:09,  3.06it/s]\rCapturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 20/49 [00:06<00:09,  3.03it/s]\rCapturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 21/49 [00:06<00:09,  3.08it/s]\rCapturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 22/49 [00:07<00:08,  3.15it/s]\rCapturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 23/49 [00:07<00:08,  3.15it/s]\rCapturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 24/49 [00:07<00:07,  3.13it/s]\rCapturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 25/49 [00:07<00:07,  3.12it/s]\rCapturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 26/49 [00:08<00:07,  3.15it/s]\rCapturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 27/49 [00:08<00:07,  3.05it/s]\rCapturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 28/49 [00:09<00:07,  2.79it/s]\rCapturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 29/49 [00:09<00:07,  2.85it/s]\rCapturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 30/49 [00:09<00:06,  2.93it/s]\rCapturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 31/49 [00:10<00:06,  2.97it/s]\rCapturing CUDA graphs (decode, FULL):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 32/49 [00:10<00:05,  3.01it/s]\rCapturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 33/49 [00:10<00:05,  3.08it/s]\rCapturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 34/49 [00:10<00:04,  3.13it/s]\rCapturing CUDA graphs (decode, FULL):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 35/49 [00:11<00:04,  3.16it/s]\rCapturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 36/49 [00:11<00:04,  3.17it/s]\rCapturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 37/49 [00:11<00:03,  3.17it/s]\rCapturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 38/49 [00:12<00:03,  3.20it/s]\rCapturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 39/49 [00:12<00:03,  3.24it/s]\rCapturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 40/49 [00:12<00:02,  3.26it/s]\rCapturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 41/49 [00:13<00:02,  3.28it/s]\rCapturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 42/49 [00:13<00:02,  3.26it/s]\rCapturing CUDA graphs (decode, FULL):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 43/49 [00:13<00:01,  3.26it/s]\rCapturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 44/49 [00:14<00:01,  3.24it/s]\rCapturing CUDA graphs (decode, FULL):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 45/49 [00:14<00:01,  3.16it/s]\rCapturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/49 [00:14<00:00,  3.12it/s]\rCapturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/49 [00:15<00:00,  3.11it/s]\rCapturing CUDA graphs (decode, FULL):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 48/49 [00:15<00:00,  3.10it/s]\rCapturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:15<00:00,  3.20it/s]\rCapturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:15<00:00,  3.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-16 19:41:57 [gpu_model_runner.py:3118] Graph capturing finished in 37 secs, took 2.54 GiB\n",
            "INFO 11-16 19:41:57 [vllm_utils.py:712] Unsloth: Patched vLLM v1 graph capture finished in 37 secs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-16 19:41:58 [gpu_worker.py:391] Free memory on device (78.72/79.25 GiB) on startup. Desired GPU memory utilization is (0.6462930630816577, 51.22 GiB). Actual usage is 2.44 GiB for weight, 2.01 GiB for peak activation, 0.02 GiB for non-torch memory, and 2.54 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=47317980569` to fit into requested memory, or `--kv-cache-memory=76851624960` to fully utilize gpu memory. Current kv cache memory in use is 50199467417 bytes.\n",
            "INFO 11-16 19:41:59 [core.py:218] init engine (profile, create kv cache, warmup model) took 71.25 seconds\n",
            "INFO 11-16 19:42:00 [llm.py:295] Supported_tasks: ('generate',)\n",
            "INFO 11-16 19:42:00 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Unsloth: Just some info: will skip parsing ['attention_norm', 'k_norm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'input_layernorm', 'ffn_norm', 'post_layernorm', 'norm', 'norm2', 'q_norm', 'layer_norm2', 'layer_norm1', 'post_feedforward_layernorm', 'norm1']\n",
            "Performing substitution for additional_keys=set()\n",
            "Unsloth: Just some info: will skip parsing ['attention_norm', 'k_norm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'input_layernorm', 'ffn_norm', 'post_layernorm', 'norm', 'norm2', 'q_norm', 'cross_attn_input_layernorm', 'layer_norm2', 'cross_attn_post_attention_layernorm', 'layer_norm1', 'post_feedforward_layernorm', 'norm1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1536  # Reduced from 2048\n",
        "lora_rank = 64\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.65,  # Changed from 0.9\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Pydantic Output Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected JSON Output Format:\n",
            "{\n",
            "  \"$defs\": {\n",
            "    \"MCQQuestion\": {\n",
            "      \"description\": \"Single Multiple Choice Question\",\n",
            "      \"properties\": {\n",
            "        \"question\": {\n",
            "          \"description\": \"The question text\",\n",
            "          \"title\": \"Question\",\n",
            "          \"type\": \"string\"\n",
            "        },\n",
            "        \"option_a\": {\n",
            "          \"description\": \"Option A\",\n",
            "          \"title\": \"Option A\",\n",
            "          \"type\": \"string\"\n",
            "        },\n",
            "        \"option_b\": {\n",
            "          \"description\": \"Option B\",\n",
            "          \"title\": \"Option B\",\n",
            "          \"type\": \"string\"\n",
            "        },\n",
            "        \"option_c\": {\n",
            "          \"description\": \"Option C\",\n",
            "          \"title\": \"Option C\",\n",
            "          \"type\": \"string\"\n",
            "        },\n",
            "        \"option_d\": {\n",
            "          \"description\": \"Option D\",\n",
            "          \"title\": \"Option D\",\n",
            "          \"type\": \"string\"\n",
            "        },\n",
            "        \"correct_answer\": {\n",
            "          \"description\": \"Correct answer: A, B, C, or D\",\n",
            "          \"title\": \"Correct Answer\",\n",
            "          \"type\": \"string\"\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"question\",\n",
            "        \"option_a\",\n",
            "        \"option_b\",\n",
            "        \"option_c\",\n",
            "        \"option_d\",\n",
            "        \"correct_answer\"\n",
            "      ],\n",
            "      \"title\": \"MCQQuestion\",\n",
            "      \"type\": \"object\"\n",
            "    }\n",
            "  },\n",
            "  \"description\": \"List of MCQ Questions\",\n",
            "  \"properties\": {\n",
            "    \"questions\": {\n",
            "      \"description\": \"List of generated questions\",\n",
            "      \"items\": {\n",
            "        \"$ref\": \"#/$defs/MCQQuestion\"\n",
            "      },\n",
            "      \"title\": \"Questions\",\n",
            "      \"type\": \"array\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"questions\"\n",
            "  ],\n",
            "  \"title\": \"MCQList\",\n",
            "  \"type\": \"object\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "class MCQQuestion(BaseModel):\n",
        "    \"\"\"Single Multiple Choice Question\"\"\"\n",
        "    question: str = Field(description=\"The question text\")\n",
        "    option_a: str = Field(description=\"Option A\")\n",
        "    option_b: str = Field(description=\"Option B\")\n",
        "    option_c: str = Field(description=\"Option C\")\n",
        "    option_d: str = Field(description=\"Option D\")\n",
        "    correct_answer: str = Field(description=\"Correct answer: A, B, C, or D\")\n",
        "\n",
        "class MCQList(BaseModel):\n",
        "    \"\"\"List of MCQ Questions\"\"\"\n",
        "    questions: List[MCQQuestion] = Field(description=\"List of generated questions\")\n",
        "\n",
        "print(\"Expected JSON Output Format:\")\n",
        "print(json.dumps(MCQList.model_json_schema(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing dataset...\n",
            "âœ“ Prepared 500 training examples\n",
            "\n",
            "Example entry:\n",
            "Prompt: Generate MCQ questions from this text:\n",
            "\n",
            "The rain had continued for a week and the flood had created a big river which were running by Nancy Brown's farm. As she tried to gather her cows to a higher gr...\n",
            "\n",
            "Expected Output: <questions>\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What did Nancy try to do before she fell over?\",\n",
            "    \"option_a\": \"Measure the depth of the river\",\n",
            "    \"option_b\": \"Look for a fallen tree trunk\",\n",
            "    \"option_c\": \"Protect her cows from being drowned\",\n",
            "    \"option_d\": \"Run away from the flooded farm\",\n",
            "    \"correct_...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import re\n",
        "import ast\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert MCQ question generator. Given a text passage, generate multiple choice questions.\n",
        "\n",
        "Output Format:\n",
        "<questions>\n",
        "[\n",
        "  {\n",
        "    \"question\": \"Question text here?\",\n",
        "    \"option_a\": \"First option\",\n",
        "    \"option_b\": \"Second option\",\n",
        "    \"option_c\": \"Third option\",\n",
        "    \"option_d\": \"Fourth option\",\n",
        "    \"correct_answer\": \"A\"\n",
        "  }\n",
        "]\n",
        "</questions>\n",
        "\"\"\"\n",
        "\n",
        "def prepare_race_dataset(split=\"test\", num_samples=500, config=\"high\"):\n",
        "    \"\"\"Convert RACE dataset to MCQ generation format\"\"\"\n",
        "    if config == \"both\":\n",
        "        dataset_high = load_dataset(\"EleutherAI/race\", \"high\", split=split)\n",
        "        dataset_middle = load_dataset(\"EleutherAI/race\", \"middle\", split=split)\n",
        "        dataset = concatenate_datasets([dataset_high, dataset_middle])\n",
        "    else:\n",
        "        dataset = load_dataset(\"EleutherAI/race\", config, split=split)\n",
        "    \n",
        "    prepared_data = []\n",
        "    \n",
        "    for idx, example in enumerate(dataset):\n",
        "        if idx >= num_samples:\n",
        "            break\n",
        "            \n",
        "        article = example['article']\n",
        "        problems = ast.literal_eval(example['problems'])\n",
        "        \n",
        "        mcq_list = []\n",
        "        for problem in problems:\n",
        "            question_text = problem['question']\n",
        "            options = problem['options']\n",
        "            answer = problem['answer']\n",
        "            \n",
        "            mcq_item = {\n",
        "                \"question\": question_text,\n",
        "                \"option_a\": options[0] if len(options) > 0 else \"\",\n",
        "                \"option_b\": options[1] if len(options) > 1 else \"\",\n",
        "                \"option_c\": options[2] if len(options) > 2 else \"\",\n",
        "                \"option_d\": options[3] if len(options) > 3 else \"\",\n",
        "                \"correct_answer\": answer\n",
        "            }\n",
        "            mcq_list.append(mcq_item)\n",
        "        \n",
        "        expected_output = \"<questions>\\n\" + json.dumps(mcq_list, indent=2) + \"\\n</questions>\"\n",
        "        \n",
        "        prepared_data.append({\n",
        "            'prompt': [\n",
        "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                {'role': 'user', 'content': f\"Generate MCQ questions from this text:\\n\\n{article}\"}\n",
        "            ],\n",
        "            'expected_output': expected_output,\n",
        "            'num_questions': len(mcq_list)\n",
        "        })\n",
        "    \n",
        "    return Dataset.from_list(prepared_data)\n",
        "\n",
        "print(\"Loading and preparing dataset...\")\n",
        "train_dataset = prepare_race_dataset(split=\"test\", num_samples=500, config=\"high\")\n",
        "\n",
        "print(f\"âœ“ Prepared {len(train_dataset)} training examples\")\n",
        "print(f\"\\nExample entry:\")\n",
        "print(f\"Prompt: {train_dataset[0]['prompt'][1]['content'][:200]}...\")\n",
        "print(f\"\\nExpected Output: {train_dataset[0]['expected_output'][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Reward Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Reward functions defined!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json_content(text: str) -> str:\n",
        "    \"\"\"Extract JSON from <questions> tags\"\"\"\n",
        "    match = re.search(r'<questions>\\s*(.+?)\\s*</questions>', text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward for proper XML format with <questions> tags\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    for response in responses:\n",
        "        reward = 0.0\n",
        "        if '<questions>' in response:\n",
        "            reward += 0.25\n",
        "        if '</questions>' in response:\n",
        "            reward += 0.25\n",
        "        rewards.append(reward)\n",
        "    return rewards\n",
        "\n",
        "def json_validity_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward for valid JSON structure\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    for response in responses:\n",
        "        json_content = extract_json_content(response)\n",
        "        try:\n",
        "            parsed = json.loads(json_content)\n",
        "            if isinstance(parsed, list):\n",
        "                rewards.append(0.5)\n",
        "            else:\n",
        "                rewards.append(0.0)\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def mcq_structure_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward for proper MCQ structure\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    required_fields = ['question', 'option_a', 'option_b', 'option_c', 'option_d', 'correct_answer']\n",
        "    \n",
        "    for response in responses:\n",
        "        json_content = extract_json_content(response)\n",
        "        try:\n",
        "            parsed = json.loads(json_content)\n",
        "            if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                first_q = parsed[0]\n",
        "                if all(field in first_q for field in required_fields):\n",
        "                    rewards.append(1.0)\n",
        "                else:\n",
        "                    rewards.append(0.0)\n",
        "            else:\n",
        "                rewards.append(0.0)\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def answer_validity_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward for correct answer format (A, B, C, or D)\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    for response in responses:\n",
        "        json_content = extract_json_content(response)\n",
        "        try:\n",
        "            parsed = json.loads(json_content)\n",
        "            if isinstance(parsed, list) and len(parsed) > 0:\n",
        "                valid_answers = sum(1 for q in parsed if q.get('correct_answer', '') in ['A', 'B', 'C', 'D'])\n",
        "                reward = valid_answers / len(parsed) * 0.5\n",
        "                rewards.append(reward)\n",
        "            else:\n",
        "                rewards.append(0.0)\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def question_count_reward_func(completions, num_questions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward for generating appropriate number of questions\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    for response, expected_count in zip(responses, num_questions):\n",
        "        json_content = extract_json_content(response)\n",
        "        try:\n",
        "            parsed = json.loads(json_content)\n",
        "            if isinstance(parsed, list):\n",
        "                actual_count = len(parsed)\n",
        "                if actual_count == expected_count:\n",
        "                    rewards.append(0.5)\n",
        "                elif abs(actual_count - expected_count) <= 1:\n",
        "                    rewards.append(0.25)\n",
        "                else:\n",
        "                    rewards.append(0.0)\n",
        "            else:\n",
        "                rewards.append(0.0)\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "print(\"âœ“ Reward functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure GRPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 4\n",
            "âœ“ Training configuration ready!\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True,\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 5,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_generations = 4,\n",
        "    max_prompt_length = 1024,\n",
        "    max_completion_length = 1024,\n",
        "    max_steps = 300,\n",
        "    save_steps = 100,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs_mcq_grpo\",\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training configuration ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train the Model\n",
        "\n",
        "**Training will take approximately 1-2 hours. Monitor the reward column - it should increase over time.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[accelerate.utils.other|WARNING]Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 3 | Total steps = 300\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,734,272 of 3,205,672,960 (3.74% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 1:01:12, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / format_reward_func / mean</th>\n",
              "      <th>rewards / format_reward_func / std</th>\n",
              "      <th>rewards / json_validity_reward_func / mean</th>\n",
              "      <th>rewards / json_validity_reward_func / std</th>\n",
              "      <th>rewards / mcq_structure_reward_func / mean</th>\n",
              "      <th>rewards / mcq_structure_reward_func / std</th>\n",
              "      <th>rewards / answer_validity_reward_func / mean</th>\n",
              "      <th>rewards / answer_validity_reward_func / std</th>\n",
              "      <th>rewards / question_count_reward_func / mean</th>\n",
              "      <th>rewards / question_count_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.293750</td>\n",
              "      <td>0.435454</td>\n",
              "      <td>420.200000</td>\n",
              "      <td>307.200000</td>\n",
              "      <td>607.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>420.200000</td>\n",
              "      <td>307.200000</td>\n",
              "      <td>607.200000</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.065311</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.145743</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>0.251576</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.166710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.475000</td>\n",
              "      <td>0.274843</td>\n",
              "      <td>425.025000</td>\n",
              "      <td>299.000000</td>\n",
              "      <td>626.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>425.025000</td>\n",
              "      <td>299.000000</td>\n",
              "      <td>626.600000</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.251576</td>\n",
              "      <td>0.206250</td>\n",
              "      <td>0.174649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.325000</td>\n",
              "      <td>0.344199</td>\n",
              "      <td>412.950000</td>\n",
              "      <td>268.400000</td>\n",
              "      <td>553.200000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>411.500006</td>\n",
              "      <td>268.400000</td>\n",
              "      <td>553.200000</td>\n",
              "      <td>0.005258</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.168313</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.247191</td>\n",
              "      <td>0.165625</td>\n",
              "      <td>0.154913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.307812</td>\n",
              "      <td>0.488550</td>\n",
              "      <td>418.687500</td>\n",
              "      <td>289.400000</td>\n",
              "      <td>622.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>418.687500</td>\n",
              "      <td>289.400000</td>\n",
              "      <td>622.600000</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.130623</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.261245</td>\n",
              "      <td>0.254688</td>\n",
              "      <td>0.241201</td>\n",
              "      <td>0.203125</td>\n",
              "      <td>0.158550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.336875</td>\n",
              "      <td>0.464764</td>\n",
              "      <td>430.087500</td>\n",
              "      <td>300.800000</td>\n",
              "      <td>701.200000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>424.431671</td>\n",
              "      <td>300.800000</td>\n",
              "      <td>652.800000</td>\n",
              "      <td>0.005259</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.468750</td>\n",
              "      <td>0.109157</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.236626</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.247191</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.167253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.296875</td>\n",
              "      <td>0.521641</td>\n",
              "      <td>422.975000</td>\n",
              "      <td>288.200000</td>\n",
              "      <td>675.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>422.975000</td>\n",
              "      <td>288.200000</td>\n",
              "      <td>675.600000</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>0.099468</td>\n",
              "      <td>0.912500</td>\n",
              "      <td>0.207756</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.249852</td>\n",
              "      <td>0.128125</td>\n",
              "      <td>0.164060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.462500</td>\n",
              "      <td>0.440523</td>\n",
              "      <td>430.800000</td>\n",
              "      <td>302.400000</td>\n",
              "      <td>645.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>430.800000</td>\n",
              "      <td>302.400000</td>\n",
              "      <td>645.200000</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.468750</td>\n",
              "      <td>0.109157</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.218313</td>\n",
              "      <td>0.387500</td>\n",
              "      <td>0.203494</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.192911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.475000</td>\n",
              "      <td>0.293438</td>\n",
              "      <td>430.725000</td>\n",
              "      <td>314.800000</td>\n",
              "      <td>576.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>430.725000</td>\n",
              "      <td>314.800000</td>\n",
              "      <td>576.200000</td>\n",
              "      <td>0.004045</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.456250</td>\n",
              "      <td>0.124468</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.145084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>0.273803</td>\n",
              "      <td>417.350000</td>\n",
              "      <td>261.200000</td>\n",
              "      <td>585.200000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>414.079169</td>\n",
              "      <td>261.200000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>0.004160</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.184375</td>\n",
              "      <td>0.157952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.687500</td>\n",
              "      <td>0.213986</td>\n",
              "      <td>384.837500</td>\n",
              "      <td>249.800000</td>\n",
              "      <td>534.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>384.837500</td>\n",
              "      <td>249.800000</td>\n",
              "      <td>534.000000</td>\n",
              "      <td>0.005144</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.174878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.671875</td>\n",
              "      <td>0.271653</td>\n",
              "      <td>390.750000</td>\n",
              "      <td>273.400000</td>\n",
              "      <td>508.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>390.750000</td>\n",
              "      <td>273.400000</td>\n",
              "      <td>508.200000</td>\n",
              "      <td>0.004237</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.246875</td>\n",
              "      <td>0.186680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.628125</td>\n",
              "      <td>0.260016</td>\n",
              "      <td>360.825000</td>\n",
              "      <td>248.200000</td>\n",
              "      <td>512.000000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>359.587506</td>\n",
              "      <td>248.200000</td>\n",
              "      <td>511.000000</td>\n",
              "      <td>0.005504</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.206250</td>\n",
              "      <td>0.187430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.684375</td>\n",
              "      <td>0.310886</td>\n",
              "      <td>330.550000</td>\n",
              "      <td>212.600000</td>\n",
              "      <td>457.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>330.550000</td>\n",
              "      <td>212.600000</td>\n",
              "      <td>457.800000</td>\n",
              "      <td>0.007729</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.180623</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.284375</td>\n",
              "      <td>0.196504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640625</td>\n",
              "      <td>0.176485</td>\n",
              "      <td>328.237500</td>\n",
              "      <td>227.200000</td>\n",
              "      <td>454.600000</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>324.346155</td>\n",
              "      <td>227.200000</td>\n",
              "      <td>454.600000</td>\n",
              "      <td>0.010030</td>\n",
              "      <td>0.490625</td>\n",
              "      <td>0.020156</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.040311</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.080623</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.040311</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.192346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.684375</td>\n",
              "      <td>0.229823</td>\n",
              "      <td>309.962500</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>309.962500</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>0.011721</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.237500</td>\n",
              "      <td>0.178236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.821875</td>\n",
              "      <td>0.139755</td>\n",
              "      <td>314.862500</td>\n",
              "      <td>162.600000</td>\n",
              "      <td>450.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>314.862500</td>\n",
              "      <td>162.600000</td>\n",
              "      <td>450.200000</td>\n",
              "      <td>0.011851</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321875</td>\n",
              "      <td>0.158073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.787500</td>\n",
              "      <td>0.122169</td>\n",
              "      <td>306.725000</td>\n",
              "      <td>186.800000</td>\n",
              "      <td>424.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>306.725000</td>\n",
              "      <td>186.800000</td>\n",
              "      <td>424.200000</td>\n",
              "      <td>0.010914</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.175974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.740625</td>\n",
              "      <td>0.172692</td>\n",
              "      <td>315.750000</td>\n",
              "      <td>205.600000</td>\n",
              "      <td>437.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>315.750000</td>\n",
              "      <td>205.600000</td>\n",
              "      <td>437.600000</td>\n",
              "      <td>0.010595</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>0.179739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.759375</td>\n",
              "      <td>0.218682</td>\n",
              "      <td>319.700000</td>\n",
              "      <td>230.400000</td>\n",
              "      <td>443.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>319.700000</td>\n",
              "      <td>230.400000</td>\n",
              "      <td>443.400000</td>\n",
              "      <td>0.010108</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.309375</td>\n",
              "      <td>0.152658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.743750</td>\n",
              "      <td>0.151973</td>\n",
              "      <td>308.750000</td>\n",
              "      <td>202.800000</td>\n",
              "      <td>441.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>308.750000</td>\n",
              "      <td>202.800000</td>\n",
              "      <td>441.600000</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.268750</td>\n",
              "      <td>0.168313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.709375</td>\n",
              "      <td>0.223260</td>\n",
              "      <td>320.175000</td>\n",
              "      <td>225.200000</td>\n",
              "      <td>447.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>320.175000</td>\n",
              "      <td>225.200000</td>\n",
              "      <td>447.800000</td>\n",
              "      <td>0.014243</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.259375</td>\n",
              "      <td>0.174739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.740625</td>\n",
              "      <td>0.176342</td>\n",
              "      <td>320.525000</td>\n",
              "      <td>192.400000</td>\n",
              "      <td>457.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>320.525000</td>\n",
              "      <td>192.400000</td>\n",
              "      <td>457.000000</td>\n",
              "      <td>0.013875</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>0.197578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.650000</td>\n",
              "      <td>0.249588</td>\n",
              "      <td>283.162500</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>393.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>283.162500</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>393.600000</td>\n",
              "      <td>0.014093</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.186626</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.262500</td>\n",
              "      <td>0.164577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.743750</td>\n",
              "      <td>0.135541</td>\n",
              "      <td>302.262500</td>\n",
              "      <td>189.200000</td>\n",
              "      <td>409.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>302.262500</td>\n",
              "      <td>189.200000</td>\n",
              "      <td>409.600000</td>\n",
              "      <td>0.014006</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.256250</td>\n",
              "      <td>0.193055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.768750</td>\n",
              "      <td>0.176973</td>\n",
              "      <td>304.162500</td>\n",
              "      <td>206.800000</td>\n",
              "      <td>411.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>304.162500</td>\n",
              "      <td>206.800000</td>\n",
              "      <td>411.400000</td>\n",
              "      <td>0.014609</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.293750</td>\n",
              "      <td>0.168819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.675000</td>\n",
              "      <td>0.331314</td>\n",
              "      <td>316.575000</td>\n",
              "      <td>205.200000</td>\n",
              "      <td>477.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>316.575000</td>\n",
              "      <td>205.200000</td>\n",
              "      <td>477.000000</td>\n",
              "      <td>0.015382</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.168313</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.173590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.690625</td>\n",
              "      <td>0.256037</td>\n",
              "      <td>304.550000</td>\n",
              "      <td>215.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>304.550000</td>\n",
              "      <td>215.000000</td>\n",
              "      <td>398.000000</td>\n",
              "      <td>0.014605</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.065311</td>\n",
              "      <td>0.271875</td>\n",
              "      <td>0.150556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.796875</td>\n",
              "      <td>0.157387</td>\n",
              "      <td>314.475000</td>\n",
              "      <td>216.000000</td>\n",
              "      <td>428.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>314.475000</td>\n",
              "      <td>216.000000</td>\n",
              "      <td>428.600000</td>\n",
              "      <td>0.015231</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.321875</td>\n",
              "      <td>0.148471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.790625</td>\n",
              "      <td>0.106768</td>\n",
              "      <td>305.437500</td>\n",
              "      <td>201.600000</td>\n",
              "      <td>442.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>305.437500</td>\n",
              "      <td>201.600000</td>\n",
              "      <td>442.200000</td>\n",
              "      <td>0.015529</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.290625</td>\n",
              "      <td>0.171827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.818750</td>\n",
              "      <td>0.162140</td>\n",
              "      <td>309.850000</td>\n",
              "      <td>228.200000</td>\n",
              "      <td>410.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>309.850000</td>\n",
              "      <td>228.200000</td>\n",
              "      <td>410.800000</td>\n",
              "      <td>0.014977</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.343750</td>\n",
              "      <td>0.151347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.718750</td>\n",
              "      <td>0.151934</td>\n",
              "      <td>321.025000</td>\n",
              "      <td>229.200000</td>\n",
              "      <td>441.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>321.025000</td>\n",
              "      <td>229.200000</td>\n",
              "      <td>441.600000</td>\n",
              "      <td>0.013222</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.193699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.728125</td>\n",
              "      <td>0.164595</td>\n",
              "      <td>315.075000</td>\n",
              "      <td>202.400000</td>\n",
              "      <td>446.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>315.075000</td>\n",
              "      <td>202.400000</td>\n",
              "      <td>446.200000</td>\n",
              "      <td>0.016233</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.253125</td>\n",
              "      <td>0.168204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.675000</td>\n",
              "      <td>0.219344</td>\n",
              "      <td>297.487500</td>\n",
              "      <td>189.200000</td>\n",
              "      <td>416.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>297.487500</td>\n",
              "      <td>189.200000</td>\n",
              "      <td>416.800000</td>\n",
              "      <td>0.015645</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.165565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.540625</td>\n",
              "      <td>0.386750</td>\n",
              "      <td>301.700000</td>\n",
              "      <td>203.800000</td>\n",
              "      <td>412.600000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>300.580835</td>\n",
              "      <td>203.800000</td>\n",
              "      <td>408.600000</td>\n",
              "      <td>0.015694</td>\n",
              "      <td>0.496875</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.456250</td>\n",
              "      <td>0.124468</td>\n",
              "      <td>0.887500</td>\n",
              "      <td>0.279558</td>\n",
              "      <td>0.456250</td>\n",
              "      <td>0.124468</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.187993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>0.171984</td>\n",
              "      <td>310.150000</td>\n",
              "      <td>211.400000</td>\n",
              "      <td>430.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>310.150000</td>\n",
              "      <td>211.400000</td>\n",
              "      <td>430.800000</td>\n",
              "      <td>0.013799</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.173251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.746875</td>\n",
              "      <td>0.213542</td>\n",
              "      <td>311.837500</td>\n",
              "      <td>226.400000</td>\n",
              "      <td>440.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>311.837500</td>\n",
              "      <td>226.400000</td>\n",
              "      <td>440.000000</td>\n",
              "      <td>0.014645</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.172371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.684375</td>\n",
              "      <td>0.262005</td>\n",
              "      <td>296.987500</td>\n",
              "      <td>181.400000</td>\n",
              "      <td>438.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>296.987500</td>\n",
              "      <td>181.400000</td>\n",
              "      <td>438.800000</td>\n",
              "      <td>0.013929</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.168313</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>0.178646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.812500</td>\n",
              "      <td>0.218441</td>\n",
              "      <td>294.150000</td>\n",
              "      <td>216.600000</td>\n",
              "      <td>382.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>294.150000</td>\n",
              "      <td>216.600000</td>\n",
              "      <td>382.000000</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.144106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.775000</td>\n",
              "      <td>0.207316</td>\n",
              "      <td>299.200000</td>\n",
              "      <td>188.600000</td>\n",
              "      <td>418.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>299.200000</td>\n",
              "      <td>188.600000</td>\n",
              "      <td>418.200000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.318750</td>\n",
              "      <td>0.163194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.662500</td>\n",
              "      <td>0.234390</td>\n",
              "      <td>294.375000</td>\n",
              "      <td>208.400000</td>\n",
              "      <td>422.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>294.375000</td>\n",
              "      <td>208.400000</td>\n",
              "      <td>422.000000</td>\n",
              "      <td>0.013283</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.040311</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.130623</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.040311</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.161196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.778125</td>\n",
              "      <td>0.240440</td>\n",
              "      <td>295.337500</td>\n",
              "      <td>196.200000</td>\n",
              "      <td>429.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>295.337500</td>\n",
              "      <td>196.200000</td>\n",
              "      <td>429.800000</td>\n",
              "      <td>0.014582</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.160895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.753125</td>\n",
              "      <td>0.204990</td>\n",
              "      <td>302.137500</td>\n",
              "      <td>219.200000</td>\n",
              "      <td>409.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>302.137500</td>\n",
              "      <td>219.200000</td>\n",
              "      <td>409.000000</td>\n",
              "      <td>0.014852</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.151507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.746875</td>\n",
              "      <td>0.154648</td>\n",
              "      <td>303.700000</td>\n",
              "      <td>222.800000</td>\n",
              "      <td>484.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>303.700000</td>\n",
              "      <td>222.800000</td>\n",
              "      <td>484.000000</td>\n",
              "      <td>0.015797</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.284375</td>\n",
              "      <td>0.183790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.753125</td>\n",
              "      <td>0.160118</td>\n",
              "      <td>290.337500</td>\n",
              "      <td>203.400000</td>\n",
              "      <td>399.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>290.337500</td>\n",
              "      <td>203.400000</td>\n",
              "      <td>399.000000</td>\n",
              "      <td>0.016228</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.290625</td>\n",
              "      <td>0.188082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.762500</td>\n",
              "      <td>0.189074</td>\n",
              "      <td>286.487500</td>\n",
              "      <td>192.400000</td>\n",
              "      <td>426.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>286.487500</td>\n",
              "      <td>192.400000</td>\n",
              "      <td>426.600000</td>\n",
              "      <td>0.017333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.118313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>0.164651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.656250</td>\n",
              "      <td>0.259425</td>\n",
              "      <td>282.887500</td>\n",
              "      <td>189.800000</td>\n",
              "      <td>382.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>282.887500</td>\n",
              "      <td>189.800000</td>\n",
              "      <td>382.400000</td>\n",
              "      <td>0.014927</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.168313</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.059157</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.177203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.792187</td>\n",
              "      <td>0.138371</td>\n",
              "      <td>290.075000</td>\n",
              "      <td>215.600000</td>\n",
              "      <td>385.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>290.075000</td>\n",
              "      <td>215.600000</td>\n",
              "      <td>385.000000</td>\n",
              "      <td>0.016180</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.492188</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.318750</td>\n",
              "      <td>0.147710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.746875</td>\n",
              "      <td>0.308096</td>\n",
              "      <td>288.237500</td>\n",
              "      <td>203.600000</td>\n",
              "      <td>367.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>288.237500</td>\n",
              "      <td>203.600000</td>\n",
              "      <td>367.200000</td>\n",
              "      <td>0.016308</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.168313</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.084157</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.160942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.801563</td>\n",
              "      <td>0.098318</td>\n",
              "      <td>292.987500</td>\n",
              "      <td>209.200000</td>\n",
              "      <td>373.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>292.987500</td>\n",
              "      <td>209.200000</td>\n",
              "      <td>373.600000</td>\n",
              "      <td>0.020413</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.498437</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>0.303125</td>\n",
              "      <td>0.159131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.781250</td>\n",
              "      <td>0.226848</td>\n",
              "      <td>293.150000</td>\n",
              "      <td>229.400000</td>\n",
              "      <td>398.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>293.150000</td>\n",
              "      <td>229.400000</td>\n",
              "      <td>398.400000</td>\n",
              "      <td>0.015286</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.331250</td>\n",
              "      <td>0.166452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.803125</td>\n",
              "      <td>0.143390</td>\n",
              "      <td>287.075000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>371.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>287.075000</td>\n",
              "      <td>203.000000</td>\n",
              "      <td>371.400000</td>\n",
              "      <td>0.017457</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.164995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.696875</td>\n",
              "      <td>0.282577</td>\n",
              "      <td>280.325000</td>\n",
              "      <td>191.400000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>280.325000</td>\n",
              "      <td>191.400000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>0.015612</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.180623</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.160137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.796875</td>\n",
              "      <td>0.107203</td>\n",
              "      <td>308.825000</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>428.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>308.825000</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>428.600000</td>\n",
              "      <td>0.016312</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.174471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>0.139866</td>\n",
              "      <td>292.350000</td>\n",
              "      <td>188.200000</td>\n",
              "      <td>402.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>292.350000</td>\n",
              "      <td>188.200000</td>\n",
              "      <td>402.000000</td>\n",
              "      <td>0.013431</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0.159459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.790625</td>\n",
              "      <td>0.131145</td>\n",
              "      <td>288.200000</td>\n",
              "      <td>222.600000</td>\n",
              "      <td>385.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>288.200000</td>\n",
              "      <td>222.600000</td>\n",
              "      <td>385.000000</td>\n",
              "      <td>0.014670</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.321875</td>\n",
              "      <td>0.165169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.737500</td>\n",
              "      <td>0.222620</td>\n",
              "      <td>299.550000</td>\n",
              "      <td>214.800000</td>\n",
              "      <td>381.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>299.550000</td>\n",
              "      <td>214.800000</td>\n",
              "      <td>381.200000</td>\n",
              "      <td>0.018014</td>\n",
              "      <td>0.493750</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.293750</td>\n",
              "      <td>0.161260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.703125</td>\n",
              "      <td>0.216321</td>\n",
              "      <td>289.350000</td>\n",
              "      <td>218.600000</td>\n",
              "      <td>418.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>289.350000</td>\n",
              "      <td>218.600000</td>\n",
              "      <td>418.800000</td>\n",
              "      <td>0.020640</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.962500</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.265625</td>\n",
              "      <td>0.166751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.756250</td>\n",
              "      <td>0.131319</td>\n",
              "      <td>285.825000</td>\n",
              "      <td>217.400000</td>\n",
              "      <td>393.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>285.825000</td>\n",
              "      <td>217.400000</td>\n",
              "      <td>393.400000</td>\n",
              "      <td>0.016349</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>0.034157</td>\n",
              "      <td>0.306250</td>\n",
              "      <td>0.162168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>0.094717</td>\n",
              "      <td>299.525000</td>\n",
              "      <td>229.800000</td>\n",
              "      <td>382.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>299.525000</td>\n",
              "      <td>229.800000</td>\n",
              "      <td>382.200000</td>\n",
              "      <td>0.011735</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.152588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>0.137140</td>\n",
              "      <td>305.812500</td>\n",
              "      <td>205.800000</td>\n",
              "      <td>431.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>305.812500</td>\n",
              "      <td>205.800000</td>\n",
              "      <td>431.200000</td>\n",
              "      <td>0.015709</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262500</td>\n",
              "      <td>0.165102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([16, 1595]) with length 1595 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 1595]) with length 1595 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([16, 1537]) with length 1537 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 1537]) with length 1537 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([16, 1546]) with length 1546 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([4, 1546]) with length 1546 > the model's max sequence length of 1536.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Training complete!\n"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        format_reward_func,\n",
        "        json_validity_reward_func,\n",
        "        mcq_structure_reward_func,\n",
        "        answer_validity_reward_func,\n",
        "        question_count_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"âœ“ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ LoRA adapters saved locally to 'mcq_grpo_lora'\n",
            "ðŸ”„ Uploading to Hugging Face Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e539679f8d404761b3b511882ef0b4cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27784699c596415e8794d1246475f3f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1c416b0a0f944ddbfbaca7804b8d7d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0311cb4604624ba483cf9a043a82a5ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...pyuxfqjgw/adapter_model.safetensors:   0%|          | 30.2kB /  479MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/mohamedashraff22/qwen2.5-3b-mcq-lora\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f5e6b7cd05643c29f3cc2922bfb53b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90feff9692e449b5a5d568adbb2b45a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0387803f929541389eee38f7b34260b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  /tmp/tmp9zeavcgk/tokenizer.json       :  97%|#########7| 11.1MB / 11.4MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LoRA adapters successfully pushed to Hugging Face Hub!\n",
            "ðŸ“¦ Model available at: https://huggingface.co/mohamedashraff22/qwen2.5-3b-mcq-lora\n",
            "ðŸ”’ Token cleared from memory\n"
          ]
        }
      ],
      "source": [
        "# Save locally\n",
        "model.save_lora(\"mcq_grpo_lora\")\n",
        "print(\"âœ“ LoRA adapters saved locally to 'mcq_grpo_lora'\")\n",
        "\n",
        "# Save to Hugging Face Hub\n",
        "# REPLACE WITH YOUR ACTUAL TOKEN - Delete this cell after successful upload for security\n",
        "HF_TOKEN = \"\"  # â† Paste your token here\n",
        "\n",
        "if HF_TOKEN and HF_TOKEN.startswith('hf_') and len(HF_TOKEN) > 10:\n",
        "    try:\n",
        "        print(\"ðŸ”„ Uploading to Hugging Face Hub...\")\n",
        "        model.push_to_hub(\"mohamedashraff22/qwen2.5-3b-mcq-lora\", token=HF_TOKEN)\n",
        "        tokenizer.push_to_hub(\"mohamedashraff22/qwen2.5-3b-mcq-lora\", token=HF_TOKEN)\n",
        "        print(\"âœ… LoRA adapters successfully pushed to Hugging Face Hub!\")\n",
        "        print(\"ðŸ“¦ Model available at: https://huggingface.co/mohamedashraff22/qwen2.5-3b-mcq-lora\")\n",
        "        \n",
        "        # Clear token from memory for security\n",
        "        HF_TOKEN = None\n",
        "        print(\"ðŸ”’ Token cleared from memory\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to push to HF: {e}\")\n",
        "        print(\"ðŸ’¡ Tip: Check if repo exists and token has write permissions\")\n",
        "else:\n",
        "    print(\"âš ï¸ Invalid token format. Skipping HF upload.\")\n",
        "    print(\"ðŸ“¥ Files saved locally to 'mcq_grpo_lora/' - download manually\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test Inference - Fine-tuned Model with LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINE-TUNED MODEL OUTPUT (With LoRA):\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "587874eae55b43709d3def5afbec7e71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75acc12e8dbe4aa4a0287d56c89d2bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|             | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<questions>\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What happened to Nancy after she slipped and hit her head?\",\n",
            "    \"option_a\": \"She fell unconscious and couldn't wake up.\",\n",
            "    \"option_b\": \"She slipped and hit her head on a fallen tree trunk.\",\n",
            "    \"option_c\": \"She gathered her cows and began walking slowly.\",\n",
            "    \"option_d\": \"She was licked by Lizzie and everything was fine.\",\n",
            "    \"correct_answer\": \"A\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Who was licking Nancy's face when she came to?\",\n",
            "    \"option_a\": \"Lizzie was licking her face.\",\n",
            "    \"option_b\": \"The flood was licking her face.\",\n",
            "    \"option_c\": \"Her friends were licking her face.\",\n",
            "    \"option_d\": \"Lizzie was licking her face.\",\n",
            "    \"correct_answer\": \"D\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"How was Nancy able to gather all her strength?\",\n",
            "    \"option_a\": \"She was already strong before the accident.\",\n",
            "    \"option_b\": \"She was helped by her friends.\",\n",
            "    \"option_c\": \"She was inspired by Lizzie.\",\n",
            "    \"option_d\": \"She was licking her face.\",\n",
            "    \"correct_answer\": \"C\"\n",
            "  }\n",
            "]\n",
            "</questions>\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "lora_adapter = model.load_lora(\"mcq_grpo_lora\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINE-TUNED MODEL OUTPUT (With LoRA):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "output_finetuned = model.fast_generate(\n",
        "    test_prompt,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = lora_adapter,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "print(output_finetuned)\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Parse and Validate Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully generated and validated MCQ questions!\n",
            "\n",
            "Generated 3 questions:\n",
            "{\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"question\": \"What happened to Nancy after she slipped and hit her head?\",\n",
            "      \"option_a\": \"She fell unconscious and couldn't wake up.\",\n",
            "      \"option_b\": \"She slipped and hit her head on a fallen tree trunk.\",\n",
            "      \"option_c\": \"She gathered her cows and began walking slowly.\",\n",
            "      \"option_d\": \"She was licked by Lizzie and everything was fine.\",\n",
            "      \"correct_answer\": \"A\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Who was licking Nancy's face when she came to?\",\n",
            "      \"option_a\": \"Lizzie was licking her face.\",\n",
            "      \"option_b\": \"The flood was licking her face.\",\n",
            "      \"option_c\": \"Her friends were licking her face.\",\n",
            "      \"option_d\": \"Lizzie was licking her face.\",\n",
            "      \"correct_answer\": \"D\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How was Nancy able to gather all her strength?\",\n",
            "      \"option_a\": \"She was already strong before the accident.\",\n",
            "      \"option_b\": \"She was helped by her friends.\",\n",
            "      \"option_c\": \"She was inspired by Lizzie.\",\n",
            "      \"option_d\": \"She was licking her face.\",\n",
            "      \"correct_answer\": \"C\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def parse_mcq_output(output_text: str) -> dict:\n",
        "    \"\"\"Parse and validate MCQ output\"\"\"\n",
        "    json_content = extract_json_content(output_text)\n",
        "    \n",
        "    try:\n",
        "        parsed = json.loads(json_content)\n",
        "        mcq_list = MCQList(questions=[MCQQuestion(**q) for q in parsed])\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"data\": mcq_list.model_dump(),\n",
        "            \"num_questions\": len(mcq_list.questions)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"raw_output\": output_text[:500]\n",
        "        }\n",
        "\n",
        "result = parse_mcq_output(output_finetuned)\n",
        "\n",
        "if result[\"success\"]:\n",
        "    print(\"âœ… Successfully generated and validated MCQ questions!\")\n",
        "    print(f\"\\nGenerated {result['num_questions']} questions:\")\n",
        "    print(json.dumps(result[\"data\"], indent=2))\n",
        "else:\n",
        "    print(\"âŒ Failed to parse output:\")\n",
        "    print(f\"Error: {result['error']}\")\n",
        "    print(f\"Raw output: {result['raw_output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test with Large Custom Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATING MCQs FROM LARGE TEXT:\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c81810b10fda41138ee251a7866536d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f6aceb0493d4853b936d89e9cc71edb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|             | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<questions>\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What is the primary goal of machine learning?\",\n",
            "    \"option_a\": \"To provide systems the ability to automatically learn and improve from experience without being explicitly programmed\",\n",
            "    \"option_b\": \"To make predictions based on existing data without learning from past examples\",\n",
            "    \"option_c\": \"To allow the computers to learn automatically without human intervention and adjust actions accordingly\",\n",
            "    \"option_d\": \"To teach computers to learn by themselves without human intervention or assistance\",\n",
            "    \"correct_answer\": \"C\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What type of machine learning algorithms can apply past learning to new data to predict future events?\",\n",
            "    \"option_a\": \"Unsupervised learning\",\n",
            "    \"option_b\": \"Supervised learning\",\n",
            "    \"option_c\": \"Reinforcement learning\",\n",
            "    \"option_d\": \"Clustering learning\",\n",
            "    \"correct_answer\": \"B\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What is unsupervised learning used for?\",\n",
            "    \"option_a\": \"To apply past learning to new data for prediction\",\n",
            "    \"option_b\": \"To classify and label data for training\",\n",
            "    \"option_c\": \"To analyze unlabeled data to find hidden structures\",\n",
            "    \"option_d\": \"To adjust actions based on past examples\",\n",
            "    \"correct_answer\": \"C\"\n",
            "  }\n",
            "]\n",
            "</questions>\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ… Successfully generated 3 questions from large text!\n",
            "ðŸ“ Saved to 'generated_mcqs.json'\n"
          ]
        }
      ],
      "source": [
        "large_text = \"\"\"Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn \n",
        "and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs \n",
        "that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as examples, \n",
        "direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples \n",
        "that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust \n",
        "actions accordingly.\n",
        "\n",
        "There are several types of machine learning algorithms. Supervised learning algorithms can apply what has been learned in the past to new data \n",
        "using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an \n",
        "inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training.\n",
        "\n",
        "Unsupervised learning is used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems \n",
        "can infer a function to describe a hidden structure from unlabeled data. The system doesn't figure out the right output, but it explores the \n",
        "data and can draw inferences from datasets to describe hidden structures from unlabeled data.\"\"\"\n",
        "\n",
        "large_prompt = tokenizer.apply_chat_template([\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": f\"Generate MCQ questions from this text:\\n\\n{large_text}\"}\n",
        "], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"GENERATING MCQs FROM LARGE TEXT:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "output_large = model.fast_generate(\n",
        "    large_prompt,\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9,\n",
        "        max_tokens = 1536,\n",
        "    ),\n",
        "    lora_request = lora_adapter,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "print(output_large)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "result_large = parse_mcq_output(output_large)\n",
        "if result_large[\"success\"]:\n",
        "    print(f\"\\nâœ… Successfully generated {result_large['num_questions']} questions from large text!\")\n",
        "    \n",
        "    with open('generated_mcqs.json', 'w') as f:\n",
        "        json.dump(result_large[\"data\"], f, indent=2)\n",
        "    print(\"ðŸ“ Saved to 'generated_mcqs.json'\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Parsing failed: {result_large['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Inference Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a43e2ec6c8441f7b4d8aba005cbe8bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dda88c9e21c47c09feb94dba6992e73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|             | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Generated MCQs:\n",
            "\n",
            "1. What does Python support?\n",
            "   A) Procedural programming only\n",
            "   B) Object-oriented programming only\n",
            "   C) Functional programming only\n",
            "   D) Procedural, object-oriented, and functional programming\n",
            "   Correct Answer: D\n",
            "\n",
            "2. Who created Python?\n",
            "   A) Guido van Rossum\n",
            "   B) Bjarne Stroustrup\n",
            "   C) James Gosling\n",
            "   D) Dennis Ritchie\n",
            "   Correct Answer: A\n",
            "\n",
            "3. When was Python first released?\n",
            "   A) 1990\n",
            "   B) 1991\n",
            "   C) 1992\n",
            "   D) 1993\n",
            "   Correct Answer: B\n"
          ]
        }
      ],
      "source": [
        "def generate_mcqs_from_text(text: str, num_questions: int = None) -> dict:\n",
        "    \"\"\"Generate MCQ questions from input text\"\"\"\n",
        "    user_content = f\"Generate MCQ questions from this text:\\n\\n{text}\"\n",
        "    if num_questions:\n",
        "        user_content += f\"\\n\\nGenerate exactly {num_questions} questions.\"\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template([\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ], tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    output = model.fast_generate(\n",
        "        prompt,\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature = 0.7,\n",
        "            top_p = 0.9,\n",
        "            max_tokens = 1536,\n",
        "        ),\n",
        "        lora_request = lora_adapter,\n",
        "    )[0].outputs[0].text\n",
        "    \n",
        "    return parse_mcq_output(output)\n",
        "\n",
        "test_text = \"\"\"Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
        "It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms, \n",
        "including procedural, object-oriented, and functional programming.\"\"\"\n",
        "\n",
        "result = generate_mcqs_from_text(test_text, num_questions=3)\n",
        "\n",
        "if result[\"success\"]:\n",
        "    print(\"âœ… Generated MCQs:\")\n",
        "    for i, q in enumerate(result[\"data\"][\"questions\"], 1):\n",
        "        print(f\"\\n{i}. {q['question']}\")\n",
        "        print(f\"   A) {q['option_a']}\")\n",
        "        print(f\"   B) {q['option_b']}\")\n",
        "        print(f\"   C) {q['option_c']}\")\n",
        "        print(f\"   D) {q['option_d']}\")\n",
        "        print(f\"   Correct Answer: {q['correct_answer']}\")\n",
        "else:\n",
        "    print(f\"âŒ Error: {result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Batch Processing - Generate MCQs from Multiple Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing text 1/2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06649a9ac9e549a493f1fb90137ff1d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f89395bee0e445ab8562d5784a3ef22d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|             | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing text 2/2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b741dea3f7ad444597b863427bb1bbd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16dbfdcab29d4abf9be20b91bf35886d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|             | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Batch processing complete! Saved to 'batch_mcqs.json'\n",
            "\n",
            "Batch Summary:\n",
            "âœ… Text 0: 3 questions generated\n",
            "âœ… Text 1: 3 questions generated\n"
          ]
        }
      ],
      "source": [
        "def batch_generate_mcqs(texts: List[str], output_file: str = \"batch_mcqs.json\") -> dict:\n",
        "    \"\"\"Generate MCQs from multiple text passages\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for idx, text in enumerate(texts):\n",
        "        print(f\"Processing text {idx+1}/{len(texts)}...\")\n",
        "        result = generate_mcqs_from_text(text)\n",
        "        \n",
        "        results.append({\n",
        "            \"text_id\": idx,\n",
        "            \"text_preview\": text[:100] + \"...\",\n",
        "            \"success\": result[\"success\"],\n",
        "            \"questions\": result.get(\"data\", {}).get(\"questions\", []),\n",
        "            \"num_questions\": result.get(\"num_questions\", 0)\n",
        "        })\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nâœ… Batch processing complete! Saved to '{output_file}'\")\n",
        "    return results\n",
        "\n",
        "sample_texts = [\n",
        "    \"\"\"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed \n",
        "    to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits \n",
        "    associated with a human mind such as learning and problem-solving.\"\"\",\n",
        "    \n",
        "    \"\"\"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with \n",
        "    representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures \n",
        "    such as deep neural networks have been applied to fields including computer vision and natural language processing.\"\"\"\n",
        "]\n",
        "\n",
        "batch_results = batch_generate_mcqs(sample_texts)\n",
        "\n",
        "print(f\"\\nBatch Summary:\")\n",
        "for result in batch_results:\n",
        "    status = \"âœ…\" if result[\"success\"] else \"âŒ\"\n",
        "    print(f\"{status} Text {result['text_id']}: {result['num_questions']} questions generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. âœ… Fine-tuning Qwen2.5-3B-Instruct with GRPO for MCQ generation\n",
        "2. âœ… Using Pydantic models for structured output validation\n",
        "3. âœ… Training on RACE dataset format\n",
        "4. âœ… Saving LoRA adapters (not merged with base model)\n",
        "5. âœ… Loading adapters from Hugging Face Hub\n",
        "6. âœ… Generating MCQs from custom text passages\n",
        "7. âœ… Batch processing multiple texts\n",
        "8. âœ… Exporting results to JSON files\n",
        "\n",
        "**Next Steps:**\n",
        "- Upload your LoRA adapters to Hugging Face Hub\n",
        "- Test with your own book chapters\n",
        "- Adjust reward functions for better quality\n",
        "- Extend training for improved performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Created mcq_grpo_lora.zip (179.51 MB)\n",
            "ðŸ“¥ Download the file from the file browser in the left sidebar\n",
            "\n",
            "Steps:\n",
            "1. Click the folder icon (ðŸ“) in the left sidebar\n",
            "2. Find 'mcq_grpo_lora.zip' in the file list\n",
            "3. Right-click â†’ Download\n"
          ]
        }
      ],
      "source": [
        "# Zip the adapter folder\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive('mcq_grpo_lora', 'zip', 'mcq_grpo_lora')\n",
        "\n",
        "# Get file size\n",
        "zip_size = os.path.getsize('mcq_grpo_lora.zip') / (1024*1024)  # MB\n",
        "print(f\"âœ“ Created mcq_grpo_lora.zip ({zip_size:.2f} MB)\")\n",
        "print(\"ðŸ“¥ Download the file from the file browser in the left sidebar\")\n",
        "print(\"\\nSteps:\")\n",
        "print(\"1. Click the folder icon (ðŸ“) in the left sidebar\")\n",
        "print(\"2. Find 'mcq_grpo_lora.zip' in the file list\")\n",
        "print(\"3. Right-click â†’ Download\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
